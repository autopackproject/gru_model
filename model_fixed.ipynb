{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from boto3 import session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_id = os.environ.get('aws_access_key_id')\n",
    "secret_key = os.environ.get('aws_secret_access_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3', aws_access_key_id=key_id, aws_secret_access_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'S3' object has no attribute 'create'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bucket_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mluyang_ai_hackathon_bucket\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43ms3_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m(Bucket\u001b[38;5;241m=\u001b[39mbucket_name)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/botocore/client.py:906\u001b[0m, in \u001b[0;36mBaseClient.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m event_response\n\u001b[0;32m--> 906\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    908\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'S3' object has no attribute 'create'"
     ]
    }
   ],
   "source": [
    "bucket_name = 'luyang_ai_hackathon_bucket'\n",
    "s3_client.create(Bucket=bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 128\n",
    "EOS_token = 129\n",
    "MAX_LENGTH = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(\"encoder input size\" + str(input.size()))\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # print(\"embedding input size\" + str(embedded.size()))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(target_tensor.size(2)):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            # print(\"target tensor size original: \" + str(target_tensor.squeeze(0).size()))\n",
    "            # print(\"target tensor size: \" + str(target_tensor[:,i].size()))\n",
    "            # print(\"target tensor size: \" + str(target_tensor[i,:].unsqueeze(1).size()))\n",
    "            # decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            decoder_input = target_tensor.squeeze(0)[:,i].unsqueeze(0).to(torch.int)# Teacher forcing\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        # print(\"decoder input size: \" + str(input.size()))\n",
    "        output = self.embedding(input)\n",
    "        # print(\"decoder embedding input size: \" + str(output.size()))\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        # print(input_tensor.size())\n",
    "        # input_tensor = input_tensor.to(torch.float32)\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor.to(torch.int).squeeze(0))\n",
    "\n",
    "        # print(\"encoder outputs: \" + str(encoder_outputs.size()))\n",
    "        # print(\"encoder hidden: \" + str(encoder_hidden.size()))\n",
    "        # print(\"target tensor: \" + str(target_tensor.size()))\n",
    "\n",
    "        encoder_outputs = encoder_outputs.to(torch.float32)\n",
    "        encoder_hidden = encoder_hidden.to(torch.float32)\n",
    "        target_tensor = target_tensor.to(torch.float32)\n",
    "        \n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        \n",
    "\n",
    "        embedding = nn.Embedding(130, 130)\n",
    "        # target_tensor = embedding(target_tensor.squeeze(0).to(torch.int)).to(torch.long)\n",
    "        # print(\"decoder output: \" + str(decoder_outputs.view(-1, decoder_outputs.size(1) * decoder_outputs.size(2)).size()))\n",
    "        # print(\"target_tensor: \" + str(target_tensor.view(-1, target_tensor.size(1) * target_tensor.size(2)).size()))\n",
    "        # print(\"decoder output: \" + str(decoder_outputs.view(-1).size()))\n",
    "        # print(\"target_tensor: \" + str(target_tensor.view(-1).size()))\n",
    "        # print(\"decoder output: \" + str(decoder_outputs.view(-1).size()))\n",
    "        # print(\"target_tensor: \" + str(target_tensor.view(-1).size()))\n",
    "        # decoder_outputs.view(-1, decoder_outputs.size(1) * decoder_outputs.size(2))\n",
    "        decoder_outputs = decoder_outputs.squeeze(0)\n",
    "        target_tensor = target_tensor.squeeze(0).squeeze(0).type(torch.LongTensor)\n",
    "        # print(\"decoder target\")\n",
    "        # print(decoder_outputs.size())\n",
    "        # print(target_tensor.size())\n",
    "        loss = criterion(\n",
    "            decoder_outputs,\n",
    "            target_tensor\n",
    "            # decoder_outputs.view(-1),\n",
    "            # target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.savefig('foo.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "unicode_list = []\n",
    "MAX_UNICODE = 128 + 2\n",
    "MAX_FILE_LENGTH = 10000\n",
    "\n",
    "# glove = GloVe(name='840B', dim=300)\n",
    "\n",
    "for i in range(0, MAX_UNICODE):\n",
    "    unicode_list.append(chr(i))\n",
    "\n",
    "class packageDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df, self.max_len = self.tokenize(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        input_data = self.df['inputs']\n",
    "        output_data = self.df['outputs']\n",
    "        return input_data, output_data\n",
    "\n",
    "    def tokenize(self, df):\n",
    "        max_len = MAX_FILE_LENGTH \n",
    "        tokenizer = get_tokenizer(\"subword\")\n",
    "        # print(df)\n",
    "        for input in df['inputs']:\n",
    "            syn_len = len(tokenizer(input))\n",
    "            if syn_len > max_len:\n",
    "                max_len = syn_len\n",
    "\n",
    "        for output in df['outputs']:\n",
    "            syn_len = len(tokenizer(output))\n",
    "            if syn_len > max_len:\n",
    "                max_len = syn_len\n",
    "\n",
    "        # for i in range(len(df['synopsis'])):\n",
    "        def process_input(ex):\n",
    "            ex = [*ex]\n",
    "            input = torch.empty((len(ex) + 1))\n",
    "            for i in range(len(ex)):\n",
    "                input[i] = torch.tensor(unicode_list.index(ex[i]))\n",
    "            input[len(ex)] = torch.tensor(129)\n",
    "            return input\n",
    "        \n",
    "        def process_output(ex):\n",
    "            ex = [*ex]\n",
    "            target = torch.empty((len(ex) + 1))\n",
    "            for i in range(len(ex)):\n",
    "                target[i] = torch.tensor(unicode_list.index(ex[i]))\n",
    "            target[len(ex)] = torch.tensor(129)\n",
    "            return target\n",
    "        \n",
    "        for i in range(len(df['inputs'])):\n",
    "            new_df = {}\n",
    "            new_inputs = process_input(df['inputs'][i])\n",
    "            new_outputs = process_output(df['outputs'][i])\n",
    "\n",
    "            new_df['inputs'] = torch.empty((len(df['inputs']), len(new_inputs)))\n",
    "            new_df['outputs'] = torch.empty((len(df['outputs']), len(new_outputs)))\n",
    "\n",
    "            new_df['inputs'][i,:] = new_inputs\n",
    "            new_df['outputs'][i,:] = new_outputs\n",
    "\n",
    "        return new_df, max_len\n",
    "\n",
    "#Data loader function\n",
    "def get_dataloader(path_to_input, path_to_output, batch_size=32):\n",
    "    df = {'inputs': [], 'outputs': []}\n",
    "\n",
    "    with open(path_to_input, 'r') as file:\n",
    "        input = file.read()\n",
    "    \n",
    "    with open(path_to_output, 'r') as file:\n",
    "        output = file.read()\n",
    "\n",
    "    df['inputs'].append(input)\n",
    "    df['outputs'].append(output)\n",
    "\n",
    "    ds = packageDataset(df)\n",
    "    max_len = ds.max_len\n",
    "\n",
    "    train_size = int(0.8*len(ds))\n",
    "    val_size = len(ds) - train_size\n",
    "    train, val = random_split(ds, [train_size, val_size])\n",
    "    return DataLoader(train, batch_size), DataLoader(val, batch_size), max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 41s (- 2m 46s) (1 20%) 4.8886\n",
      "1m 22s (- 2m 3s) (2 40%) 4.7890\n",
      "2m 2s (- 1m 21s) (3 60%) 4.6914\n",
      "2m 43s (- 0m 40s) (4 80%) 4.5918\n",
      "3m 24s (- 0m 0s) (5 100%) 4.4871\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 1\n",
    "input_size = 130\n",
    "\n",
    "train_set, val, max_len = get_dataloader('input_data/ecal/ecal_data.txt', 'output_data/ecal/ecal.spec')\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, MAX_UNICODE).to(device)\n",
    "\n",
    "train(train_set, encoder, decoder, 5, print_every=1, plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
